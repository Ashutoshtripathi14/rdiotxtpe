{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from math import *\n",
    "from numpy import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import os\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score,recall_score,precision_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "import nltk.data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data = pd.read_csv('./train_pe.csv')\n",
    "del data['Identifier']\n",
    "workabledata=data.copy()\n",
    "def striprtf(text):\n",
    "   pattern = re.compile(r\"\\\\([a-z]{1,32})(-?\\d{1,10})?[ ]?|\\\\'([0-9a-f]{2})|\\\\([^a-z])|([{}])|[\\r\\n]+|(.)\", re.I)\n",
    "   # control words which specify a \"destionation\".\n",
    "   destinations = frozenset((\n",
    "      'aftncn','aftnsep','aftnsepc','annotation','atnauthor','atndate','atnicn','atnid',\n",
    "      'atnparent','atnref','atntime','atrfend','atrfstart','author','background',\n",
    "      'bkmkend','bkmkstart','blipuid','buptim','category','colorschememapping',\n",
    "      'colortbl','comment','company','creatim','datafield','datastore','defchp','defpap',\n",
    "      'do','doccomm','docvar','dptxbxtext','ebcend','ebcstart','factoidname','falt',\n",
    "      'fchars','ffdeftext','ffentrymcr','ffexitmcr','ffformat','ffhelptext','ffl',\n",
    "      'ffname','ffstattext','field','file','filetbl','fldinst','fldrslt','fldtype',\n",
    "      'fname','fontemb','fontfile','fonttbl','footer','footerf','footerl','footerr',\n",
    "      'footnote','formfield','ftncn','ftnsep','ftnsepc','g','generator','gridtbl',\n",
    "      'header','headerf','headerl','headerr','hl','hlfr','hlinkbase','hlloc','hlsrc',\n",
    "      'hsv','htmltag','info','keycode','keywords','latentstyles','lchars','levelnumbers',\n",
    "      'leveltext','lfolevel','linkval','list','listlevel','listname','listoverride',\n",
    "      'listoverridetable','listpicture','liststylename','listtable','listtext',\n",
    "      'lsdlockedexcept','macc','maccPr','mailmerge','maln','malnScr','manager','margPr',\n",
    "      'mbar','mbarPr','mbaseJc','mbegChr','mborderBox','mborderBoxPr','mbox','mboxPr',\n",
    "      'mchr','mcount','mctrlPr','md','mdeg','mdegHide','mden','mdiff','mdPr','me',\n",
    "      'mendChr','meqArr','meqArrPr','mf','mfName','mfPr','mfunc','mfuncPr','mgroupChr',\n",
    "      'mgroupChrPr','mgrow','mhideBot','mhideLeft','mhideRight','mhideTop','mhtmltag',\n",
    "      'mlim','mlimloc','mlimlow','mlimlowPr','mlimupp','mlimuppPr','mm','mmaddfieldname',\n",
    "      'mmath','mmathPict','mmathPr','mmaxdist','mmc','mmcJc','mmconnectstr',\n",
    "      'mmconnectstrdata','mmcPr','mmcs','mmdatasource','mmheadersource','mmmailsubject',\n",
    "      'mmodso','mmodsofilter','mmodsofldmpdata','mmodsomappedname','mmodsoname',\n",
    "      'mmodsorecipdata','mmodsosort','mmodsosrc','mmodsotable','mmodsoudl',\n",
    "      'mmodsoudldata','mmodsouniquetag','mmPr','mmquery','mmr','mnary','mnaryPr',\n",
    "      'mnoBreak','mnum','mobjDist','moMath','moMathPara','moMathParaPr','mopEmu',\n",
    "      'mphant','mphantPr','mplcHide','mpos','mr','mrad','mradPr','mrPr','msepChr',\n",
    "      'mshow','mshp','msPre','msPrePr','msSub','msSubPr','msSubSup','msSubSupPr','msSup',\n",
    "      'msSupPr','mstrikeBLTR','mstrikeH','mstrikeTLBR','mstrikeV','msub','msubHide',\n",
    "      'msup','msupHide','mtransp','mtype','mvertJc','mvfmf','mvfml','mvtof','mvtol',\n",
    "      'mzeroAsc','mzeroDesc','mzeroWid','nesttableprops','nextfile','nonesttables',\n",
    "      'objalias','objclass','objdata','object','objname','objsect','objtime','oldcprops',\n",
    "      'oldpprops','oldsprops','oldtprops','oleclsid','operator','panose','password',\n",
    "      'passwordhash','pgp','pgptbl','picprop','pict','pn','pnseclvl','pntext','pntxta',\n",
    "      'pntxtb','printim','private','propname','protend','protstart','protusertbl','pxe',\n",
    "      'result','revtbl','revtim','rsidtbl','rxe','shp','shpgrp','shpinst',\n",
    "      'shppict','shprslt','shptxt','sn','sp','staticval','stylesheet','subject','sv',\n",
    "      'svb','tc','template','themedata','title','txe','ud','upr','userprops',\n",
    "      'wgrffmtfilter','windowcaption','writereservation','writereservhash','xe','xform',\n",
    "      'xmlattrname','xmlattrvalue','xmlclose','xmlname','xmlnstbl',\n",
    "      'xmlopen',\n",
    "   ))\n",
    "   # Translation of some special characters.\n",
    "   specialchars = {\n",
    "      'par': '\\n',\n",
    "      'sect': '\\n\\n',\n",
    "      'page': '\\n\\n',\n",
    "      'line': '\\n',\n",
    "      'tab': '\\t',\n",
    "      'emdash': u'\\u2014',\n",
    "      'endash': u'\\u2013',\n",
    "      'emspace': u'\\u2003',\n",
    "      'enspace': u'\\u2002',\n",
    "      'qmspace': u'\\u2005',\n",
    "      'bullet': u'\\u2022',\n",
    "      'lquote': u'\\u2018',\n",
    "      'rquote': u'\\u2019',\n",
    "      'ldblquote': u'\\201C',\n",
    "      'rdblquote': u'\\u201D', \n",
    "   }\n",
    "   stack = []\n",
    "   ignorable = False       # Whether this group (and all inside it) are \"ignorable\".\n",
    "   ucskip = 1              # Number of ASCII characters to skip after a unicode character.\n",
    "   curskip = 0             # Number of ASCII characters left to skip\n",
    "   out = []                # Output buffer.\n",
    "   for match in pattern.finditer(text):\n",
    "      word,arg,hex,char,brace,tchar = match.groups()\n",
    "      if brace:\n",
    "         curskip = 0\n",
    "         if brace == '{':\n",
    "            # Push state\n",
    "            stack.append((ucskip,ignorable))\n",
    "         elif brace == '}':\n",
    "            # Pop state\n",
    "            ucskip,ignorable = stack.pop()\n",
    "      elif char: # \\x (not a letter)\n",
    "         curskip = 0\n",
    "         if char == '~':\n",
    "            if not ignorable:\n",
    "                out.append(u'\\xA0')\n",
    "         elif char in '{}\\\\':\n",
    "            if not ignorable:\n",
    "               out.append(char)\n",
    "         elif char == '*':\n",
    "            ignorable = True\n",
    "      elif word: # \\foo\n",
    "         curskip = 0\n",
    "         if word in destinations:\n",
    "            ignorable = True\n",
    "         elif ignorable:\n",
    "            pass\n",
    "         elif word in specialchars:\n",
    "            out.append(specialchars[word])\n",
    "         elif word == 'uc':\n",
    "            ucskip = int(arg)\n",
    "         elif word == 'u':\n",
    "            c = int(arg)\n",
    "            if c < 0: c += 0x10000\n",
    "            if c > 127: out.append(chr(c))\n",
    "            else: out.append(chr(c))\n",
    "            curskip = ucskip\n",
    "      elif hex: # \\'xx\n",
    "         if curskip > 0:\n",
    "            curskip -= 1\n",
    "         elif not ignorable:\n",
    "            c = int(hex,16)\n",
    "            if c > 127: out.append(chr(c))\n",
    "            else: out.append(chr(c))\n",
    "      elif tchar:\n",
    "         if curskip > 0:\n",
    "            curskip -= 1\n",
    "         elif not ignorable:\n",
    "            out.append(tchar)\n",
    "   return ''.join(out)\n",
    "twword=[('mid','zones'),('pleural', 'effusion'),('right', 'lobe'),('lower', 'lobe'),('middle', 'lobe'),('upper', 'lobe'),('left', 'lobe'),('right', 'side'),('left', 'side'),('left', 'kidney'),('right', 'kidney'),('wall', 'thickening'),('bilateral', 'kidneys'),('focal', 'lesion'),('urinary', 'bladder'),('gall', 'bladder'),('mass', 'seen'),('wall', 'oedema'),('lymph', 'nodes'),('thick', 'walled'),('free', 'fluid'),('soft', 'tissues'),('bladder', 'wall')]\n",
    "oofsdfd=[]\n",
    "for x in twword:\n",
    "    a=x[0]+'_'+x[1]\n",
    "    oofsdfd.append(a)\n",
    "df = pd.read_csv(\"train_pe.csv\", usecols=[2])\n",
    "for i,j in df.iterrows(): \n",
    "    text=df.Result[i]\n",
    "    df.Result[i]=striprtf(text)\n",
    "for i,j in df.iterrows(): \n",
    "    text=df.Result[i]\n",
    "    df.Result[i]=text.lower()\n",
    "to_remove = ['no', 'nor','there','is']\n",
    "new_stopwords = set(stopwords.words('english')).difference(to_remove)\n",
    "for i,j in df.iterrows():                  \n",
    "    text=df.Result[i]\n",
    "    words=text.split()\n",
    "    meaningful_words = [w for w in words if not w in new_stopwords]   \n",
    "    df.Result[i]=(\" \".join( meaningful_words))\n",
    "test_related_pe=['CT Thorax / Chest - HRCT Contrast','CT Thorax / Chest - HRCT Plain','CT Thorax / Chest - Plain','CT Whole Abdomen - Plain','CT Whole Abdomen with Contrast','CT Whole Thorax (Contrast)','CT Whole Thorax (Plain)','MRI Cholangiography (MRCP)','Ultrasound Chest','Ultrasound KUB','Ultrasound Whole Abdomen','USG Chest','USG Upper Abdomen','USG Whole Abdomen','X-ray (any Portable single exposure)','X-Ray Chest PA/AP View']\n",
    "twword=[('mid','zones'),('pleural', 'effusion'),('right', 'lobe'),('lower', 'lobe'),('middle', 'lobe'),('upper', 'lobe'),('left', 'lobe'),('right', 'side'),('left', 'side'),('left', 'kidney'),('right', 'kidney'),('wall', 'thickening'),('bilateral', 'kidneys'),('focal', 'lesion'),('urinary', 'bladder'),('gall', 'bladder'),('mass', 'seen'),('wall', 'oedema'),('lymph', 'nodes'),('thick', 'walled'),('free', 'fluid'),('soft', 'tissues'),('bladder', 'wall')]\n",
    "thrword=[('left', 'upper_lobe'),('right', 'upper_lobe'),('right','pleural_effusion'),('gall_bladder', 'walls'),('left', 'pleural_effusion')]\n",
    "s=workabledata.copy()\n",
    "sd=workabledata[workabledata.Pleural_effusion>0]\n",
    "sdt=sd.copy()\n",
    "ops=1\n",
    "sd = sd.reset_index(drop=True)\n",
    "\n",
    "top_N = 20\n",
    "txt = sd.Result.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "words_except_stop_dist = nltk.FreqDist(w for w in words if (w not in new_stopwords and w not in string.punctuation and w!='.') ) \n",
    "rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "#matplotlib.style.use('ggplot')\n",
    "word_counter = Counter(words_except_stop_dist)\n",
    "words\n",
    "oofsd=Counter(nltk.bigrams(words))\n",
    "oofsd.most_common()\n",
    "oofsdf=[]\n",
    "for x in oofsd:\n",
    "    txt=x\n",
    "    if(txt[0]=='no'):\n",
    "        oofsdf.append(txt)\n",
    "del s['Result']\n",
    "s=pd.concat([s,df],axis=1)\n",
    "okbad=[\"x-ray - chest (portable) results\",\"hrct chest (non contrast)\",\"chest x-ray pa view \",\"usg whole abdomen\",\"results\",\"report mri brain\",\"ncct scan chest\",\"usg knee\",\"usg\",\"cect head\",\"ct pns axial,coronal & sag\",\"ct whole abdomen\",\"ct thorax/chest\",\"mri brain plain\",\"x-ray abdomen\",'ncct kub','nct head','nephrogram','voluetric scanning chest','mr angio brain','cect scan thorax','usg chest','usg guided','ultrasoung guided pigtail','cect thorax & abdomen','ct scan pulmonary angio','usg pelvis','trus','x-ray ankle','colour doppler','mri report lower adomen','ultrasound doppler left upper limb arterial','arterial colour doppler','venous doppler','mri report','ct head/brain','ct thorax/chest','mri brain screening','usg breasts','mammography','x-ray left foot joint','x-ray pelvis hips','x-ray right foot joint','x-ray knee','usg','x-ray','ct kub','mri lumbo-sacral spine','mri screening','mr cholangioagraphy','x-ray lumbar spine','x-ray lumar spine','usg screening','cect pns','ct aortogram','kub','chest paap view']\n",
    "badie=['report','reports','result','results','advise','investigation','investigations','study','please correlate clinically',' please correlate clinically.']\n",
    "moth=['aug','jun''july']\n",
    "for i,j in s.iterrows():\n",
    "    text=s.Result[i]\n",
    "    for y in okbad:\n",
    "        if(y in text):\n",
    "            text=text.replace(y,'')\n",
    "    for y in badie:\n",
    "        if(y in text):\n",
    "            text=text.replace(y,'')\n",
    "    for y in moth:\n",
    "        if(y in text):\n",
    "            text=text.replace(y,'')\n",
    "    df.Result[i]=text\n",
    "\n",
    "          \n",
    "del s['Result']\n",
    "s=pd.concat([s,df],axis=1)\n",
    "test_related_pe=['CT Thorax / Chest - HRCT Contrast','CT Thorax / Chest - HRCT Plain','CT Thorax / Chest - Plain','CT Whole Abdomen - Plain','CT Whole Abdomen with Contrast','CT Whole Thorax (Contrast)','CT Whole Thorax (Plain)','MRI Cholangiography (MRCP)','Ultrasound Chest','Ultrasound KUB','Ultrasound Whole Abdomen','USG Chest','USG Upper Abdomen','USG Whole Abdomen','X-ray (any Portable single exposure)','X-Ray Chest PA/AP View']\n",
    "twword=[('pleural','cavity'),('pleural', 'effusion'),('right', 'lobe'),('lower', 'lobe'),('middle', 'lobe'),('upper', 'lobe'),('left', 'lobe'),('right', 'side'),('left', 'side'),('bilateral','cp'),('cp', 'angle'),('free', 'fluid'),('subpulmonic','effusion')]\n",
    "thrword=[('left','pleural_cavity'),('right','pleural_cavity'),('right','cp_angle'),('left','cp_angle'),('left', 'upper_lobe'),('right', 'upper_lobe'),('right','pleural_effusion'),('bilateral', 'pleural_effusion'),('left', 'pleural_effusion')]\n",
    "fourword=[('cp_angle','free_fluid'),('right','sided')]\n",
    "klo=[('right','sided'),('left','sided'),('right', 'side'),('left', 'side'),('moderate','sized'),('there','is'),]\n",
    "to2=[]\n",
    "ps = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "for i,j in df.iterrows():\n",
    "    text=df.Result[i]\n",
    "    to=nltk.tokenize.word_tokenize(text)\n",
    "    for x in range (len(to)):\n",
    "        y=to[x]\n",
    "        to[x]=lemmatizer.lemmatize(y)\n",
    "    txt=' '.join(to)\n",
    "    pattern = '[0-9]'\n",
    "    txt2 = [re.sub(pattern, '', i) for i in to]\n",
    "    txt3=' '.join(txt2)\n",
    "    df.Result[i]=txt3\n",
    "    \n",
    "for i,j in s.iterrows():\n",
    "    text=df.Result[i]\n",
    "    to=nltk.tokenize.word_tokenize(text)\n",
    "    oof=[]\n",
    "    for x in range(len(to)-1):\n",
    "        a=(to[x],to[x+1])\n",
    "        if(a in twword):\n",
    "            oof.append(x)\n",
    "    for x in oof:\n",
    "        a=to[x]+'_'+to[x+1]\n",
    "        to[x]=a\n",
    "        to[x+1]=''\n",
    "\n",
    "    oof2=[]\n",
    "    for x in range(len(to)-1):\n",
    "        a=(to[x],to[x+1])\n",
    "        if(a in thrword):\n",
    "            #print(a)\n",
    "            oof2.append(x)\n",
    "    for x in oof2:\n",
    "        a=to[x]+'_'+to[x+1]\n",
    "        to[x]=a\n",
    "        to[x+1]=''\n",
    "    oof3=[]\n",
    "    for x in range(len(to)-1):\n",
    "        a=(to[x],to[x+1])\n",
    "        if(a in fourword):\n",
    "            oof3.append(x)\n",
    "    for x in oof3:\n",
    "        a=to[x]+'_'+to[x+1]\n",
    "        to[x]=a\n",
    "        to[x+1]=''\n",
    "    oof3=[]\n",
    "    for x in range(len(to)-1):\n",
    "        a=(to[x],to[x+1])\n",
    "        if(a in klo):\n",
    "            oof3.append(x)\n",
    "    for x in oof3:\n",
    "        a=to[x]+'_'+to[x+1]\n",
    "        to[x]=a\n",
    "        to[x+1]=''\n",
    "    oof3=[]\n",
    "    for x in range(len(to)-1):\n",
    "        a=(to[x],to[x+1])\n",
    "        if(a in oofsdf):\n",
    "            oof3.append(x)\n",
    "    for x in oof3:\n",
    "        a=to[x]+'_'+to[x+1]\n",
    "        to[x]=a\n",
    "        to[x+1]=''\n",
    "\n",
    "    for x in to:\n",
    "        if(x==''):\n",
    "            to.remove(x)\n",
    "    txt=' '.join(to)\n",
    "    df.Result[i]=txt\n",
    "\n",
    "ps = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "for i,j in df.iterrows():\n",
    "    text=df.Result[i]\n",
    "    to=nltk.tokenize.word_tokenize(text)\n",
    "    for x in range (len(to)):\n",
    "        y=to[x]\n",
    "        to[x]=lemmatizer.lemmatize(y)\n",
    "    txt=' '.join(to)\n",
    "    pattern = '[0-9]'\n",
    "    txt2 = [re.sub(pattern, '', i) for i in to]\n",
    "    txt3=' '.join(txt2)\n",
    "    df.Result[i]=txt3\n",
    "del s['Result']\n",
    "s=pd.concat([s,df],axis=1)\n",
    "\n",
    "test_related_pe=['CT Thorax / Chest - HRCT Contrast','CT Thorax / Chest - HRCT Plain','CT Thorax / Chest - Plain','CT Whole Abdomen - Plain','CT Whole Abdomen with Contrast','CT Whole Thorax (Contrast)','CT Whole Thorax (Plain)','MRI Cholangiography (MRCP)','Ultrasound Chest','Ultrasound KUB','Ultrasound Whole Abdomen','USG Chest','USG Upper Abdomen','USG Whole Abdomen','X-ray (any Portable single exposure)','X-Ray Chest PA/AP View']\n",
    "twword=[('pleural','cavity'),('pleural', 'effusion'),('bilateral','cp'),('cp', 'angle'),('free', 'fluid')]\n",
    "thrword=[('left','pleural_cavity'),('right','pleural_cavity'),('right','cp_angle'),('left','cp_angle'),('right','pleural_effusion'),('bilateral', 'pleural_effusion'),('left', 'pleural_effusion')]\n",
    "fourword=[('cp_angle','free_fluid')]\n",
    "safe=['clear','no_evidence','no_significant']\n",
    "ad2j=['right_sided','left_sided','right_side','left_side','bilateral']\n",
    "commonevidencesuggestive=['present','minimal','present','seen','hazy','due to','s/o','mild','evidence','moderate_sized','noted','?','??']\n",
    "wordsrelatedtope=['pleural_effusion','bilateral_cp','free_fluid','cp_angle_free_fluid','bilateral_pleural_effusion','left_pleural_effusion','right_pleural_effusion','bilateral_pleural_effusion','cp_angle','left_cp_angle','right_cp_angle','pleural_cavity','left_pleural_cavity','right_pleural_cavity','subpulmonic_effusion','effusion']\n",
    "okdokie=[]\n",
    "for x in twword:\n",
    "    a=x[0]+'_'+x[1]\n",
    "    okdokie.append(a)\n",
    "for x in thrword:\n",
    "    a=x[0]+'_'+x[1]\n",
    "    okdokie.append(a)\n",
    "for x in fourword:\n",
    "    a=x[0]+'_'+x[1]\n",
    "    okdokie.append(a)   \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "n=0\n",
    "u=[]\n",
    "for i,j in s.iterrows():\n",
    "    text=s.Result[i]\n",
    "    to=tokenizer.tokenize(text)\n",
    "    if(s.Pleural_effusion[i]==1 or s.Pleural_effusion[i]==0):\n",
    "        for z in range(len(to)): \n",
    "            replace=0\n",
    "            toy=to[z]\n",
    "            oofdiedoofdie=nltk.tokenize.word_tokenize(toy)\n",
    "            for x in range(len(oofdiedoofdie)):\n",
    "                os=oofdiedoofdie[x]\n",
    "                for y in okdokie:\n",
    "                    if(os==y):\n",
    "                        replace+=1\n",
    "                        oofdiedoofdie[x]='FTR'\n",
    "                for y in safe:\n",
    "                    if(os==y):\n",
    "                        replace+=1\n",
    "                        oofdiedoofdie[x]='SFE'\n",
    "                for y in ad2j:\n",
    "                    #print(y)\n",
    "                    if(os==y):\n",
    "                        replace+=1\n",
    "                        #print(y)\n",
    "                        oofdiedoofdie[x]='ADJ'\n",
    "                for y in commonevidencesuggestive:\n",
    "                    if(os==y):\n",
    "                        replace+=1\n",
    "                        oofdiedoofdie[x]='RISK'\n",
    "                for y in wordsrelatedtope:\n",
    "                    if(os==y):\n",
    "                        replace+=1\n",
    "                        oofdiedoofdie[x]='FTR'\n",
    "            to[z]=' '.join(oofdiedoofdie)\n",
    "    text=' '.join(to)\n",
    "    df.Result[i]=text\n",
    "del s['Result']\n",
    "s=pd.concat([s,df],axis=1)\n",
    "sop=s.copy()\n",
    "oof1=0\n",
    "oof2=0\n",
    "totsd=0\n",
    "a=len()\n",
    "for i,j in s.iterrows():\n",
    "    text=s.Result[i]\n",
    "    to=tokenizer.tokenize(text)\n",
    "    flag=0\n",
    "    \n",
    "print(oof1,oof2,totsd)\n",
    "import re\n",
    "unique_words=set()\n",
    "punctuations = list(string.punctuation)\n",
    "for i,j in df.iterrows():\n",
    "    text=df.Result[i]\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*~+/='''\n",
    "    text3=\"\"\n",
    "    for char in text:\n",
    "        if char not in punctuations:\n",
    "            text3 = text3 + char\n",
    "    unique_words_temp=set(text3.split())\n",
    "    unique_words=unique_words.union(unique_words_temp)\n",
    "    df.Result[i]=text3.lstrip()\n",
    "    \n",
    "del s['Result']\n",
    "s=pd.concat([s,df],axis=1)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "new_stopwords = set(stopwords.words('english')).difference(to_remove)\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=new_stopwords)\n",
    "matrix = vectorizer.fit_transform(s.Result)\n",
    "matrix=matrix.toarray()\n",
    "x=matrix\n",
    "y=s.Pleural_effusion\n",
    "yt=y.array\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.10,random_state=1)\n",
    "print(x_train)\n",
    "\n",
    "print(\"-----Next Linear SVC-----\")\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(x_train, y_train)  \n",
    "y_pred_SVM=clf.predict(x_test)\n",
    "print (\"Accuracy :\",accuracy_score(y_test,y_pred_SVM))\n",
    "print (\"F1;:\",f1_score(y_test,y_pred_SVM))\n",
    "print(\"Precision:\",precision_score(y_test,y_pred_SVM))\n",
    "print(\"Recall:\",recall_score(y_test,y_pred_SVM))\n",
    "print(\"Area Under Curve:\",roc_auc_score(y_test,y_pred_SVM))\n",
    "cm = confusion_matrix(y_test,y_pred_SVM)\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity=tn / (tn+fp)\n",
    "sensitivity=tp / (tp+fn)\n",
    "print(\"Specificity:\",specificity)\n",
    "print(\"Sensitivity:\",sensitivity)\n",
    "olala=y_pred_SVM\n",
    "y_pred_SVM=clf.predict(x_train)\n",
    "print (\"Accuracy :\",accuracy_score(y_train,y_pred_SVM))\n",
    "print (\"F1;:\",f1_score(y_train,y_pred_SVM))\n",
    "print(\"Precision:\",precision_score(y_train,y_pred_SVM))\n",
    "print(\"Recall:\",recall_score(y_train,y_pred_SVM))\n",
    "print(\"Area Under Curve:\",roc_auc_score(y_train,y_pred_SVM))\n",
    "cm = confusion_matrix(y_train,y_pred_SVM)\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity=tn / (tn+fp)\n",
    "sensitivity=tp / (tp+fn)\n",
    "print(\"Specificity:\",specificity)\n",
    "print(\"Sensitivity:\",sensitivity)\n",
    "olala=y_pred_SVM\n",
    "print(\"-----Next Logistic-----\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lor=LogisticRegression()\n",
    "lor.fit(x_train,y_train)\n",
    "ospdo=lor.predict(x_test)\n",
    "print (\"Accuracy :\",accuracy_score(y_test,ospdo))\n",
    "print (\"F1;:\",f1_score(y_test,ospdo))\n",
    "print(\"Precision:\",precision_score(y_test,ospdo))\n",
    "print(\"Recall:\",recall_score(y_test,ospdo))\n",
    "cm = confusion_matrix(y_test,ospdo)\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity=tn / (tn+fp)\n",
    "sensitivity=tp / (tp+fn)\n",
    "print(\"Specificity:\",specificity)\n",
    "print(\"Sensitivity:\",sensitivity)\n",
    "opsie=ospdo\n",
    "ohopsie=[]\n",
    "for i in range(len(opsie)):\n",
    "    ole=opsie[i]\n",
    "    ola=olala[i]\n",
    "    a=(ola or ole)\n",
    "    ohopsie.append(a)\n",
    "print(\"-----Next ensemble-----\") \n",
    "print (\"Accuracy :\",accuracy_score(y_test,ohopsie))\n",
    "print (\"F1;:\",f1_score(y_test,ohopsie))\n",
    "print(\"Precision:\",precision_score(y_test,ohopsie))\n",
    "print(\"Recall:\",recall_score(y_test,ohopsie))\n",
    "cm = confusion_matrix(y_test,ohopsie)\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity=tn / (tn+fp)\n",
    "sensitivity=tp / (tp+fn)\n",
    "print(\"Specificity:\",specificity)\n",
    "print(\"Sensitivity:\",sensitivity)  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
